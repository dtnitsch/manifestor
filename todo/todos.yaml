project: scanner
version: 0.2
last_updated: 2025-12-30

priorities:
  p0:
    description: "Must-have for next release; unlocks real reasoning value"
    items:
      - id: rollup-size-stats
        title: "Directory size statistics"
        status: complete
        details:
          - total_size_bytes
          - min_file_size
          - max_file_size
          - mean_file_size
          - median_file_size
        rationale: "Enables cost estimation, monster file detection, ingestion safety"

      - id: rollup-size-buckets
        title: "File size buckets"
        status: planned
        details:
          - "<1KB"
          - "1KB-1MB"
          - "1MB-10MB"
          - ">10MB"
        rationale: "Fast binary/blob detection without rescanning"

      - id: rollup-activity-span
        title: "Modification time span"
        status: planned
        details:
          - oldest_mtime
          - newest_mtime
          - span_seconds
        rationale: "Identifies active vs dead directories"

      - id: capability-integrity
        title: "Capability-driven invariant enforcement"
        status: in-progress
        details:
          - manifest-declared capabilities
          - table-driven validation
          - per-capability failure reporting
        rationale: "Prevents silent data corruption and enables safe downstream reasoning"

      - id: rollup-completeness
        title: "Rollup completeness indicators"
        status: planned
        details:
          - directories_with_rollups
          - directories_missing_rollups
        rationale: "Allows consumers to reason about partial manifests"

  p1:
    description: "High signal, low additional cost"
    items:
      - id: extension-entropy
        title: "Extension entropy metric"
        status: planned
        rationale: "Detects mixed-modality directories and summarization complexity"

      - id: structure-depth
        title: "Directory depth metrics"
        status: planned
        details:
          - max_depth
          - avg_depth
        rationale: "Distinguishes wide vs deep trees for chunking strategies"

      - id: schema-evolution-policy
        title: "Schema and capability evolution rules"
        status: planned
        rationale: "Ensures backward compatibility and safe consumer upgrades"

  p2:
    description: "Nice to have; supports smarter routing later"
    items:
      - id: text-binary-ratio
        title: "Text vs binary file counts"
        status: will-not-do
        rationale: "Prevents wasted LLM tokens; improves ingestion decisions"

      - id: largest-files
        title: "Top-N largest files"
        status: planned
        details:
          - limit: 3
        rationale: "Explains skewed stats and aids human debugging"

  p3:
    description: "Deferred / future"
    items:
      - id: hotspot-score
        title: "Heuristic hotspot score"
        status: deferred
        rationale: "Useful for UI and triage but requires tuning"

notes:
  - "All rollup metrics must be derivable in a single scan pass"
  - "No file content reads beyond stat() in v0.2"
  - "Rollup must remain cheap enough to run on large repos"



  ---


recommendations from claude:

 Absolutely integrate manifest.json into the LLM context system!

  Phase 2 (v0.4)

  - Add table metadata to migration file entries
  - Add semantic grouping for doc categories
  - Use manifest to auto-generate the schema-overview file

  Phase 3 (v0.5)

  - Split manifest by directory (as you mentioned)
  - Add dependency graphs
  - Make manifest-based trigger rules more sophisticated

  ---
  Final Answer

  YES - This would save MASSIVE tokens! The manifest is like a "table of contents" that lets the LLM make
  intelligent decisions about what to read before wasting tokens reading it.

  Your hard question example would go from ~3,500 tokens to potentially 250 tokens if the manifest had
  semantic metadata.

  This is exactly the kind of infrastructure-level optimization that makes LLM-assisted development actually
  feasible at scale.


---------


Based on my experience with the TODO.md verification, here are high-value additions:

  Tier 1: Quick Wins (High Value, Low Complexity)

  1. File Headers Snapshot â­ MOST VALUABLE

  {
    "path": "internal/requestingsource/postgres.go",
    "header_lines": [
      "package requestingsource",
      "import (",
      "  \"context\"",
      "  ...",
      ")",
      "type PostgresRepository struct {",
      "  pool *pgxpool.Pool",
      "  ..."
    ]
  }
  Why: Would have instantly shown pool *pgxpool.Pool vs db DBTX without reading files. This is 80% of
  verification tasks.

  Implementation: First 20-30 lines per file (captures package, imports, top-level types).

  ---
  2. Build Tags & Test Markers

  {
    "path": "internal/database/tx_logger_test.go",
    "build_tags": ["integration"],
    "test_file": true,
    "test_for": "internal/database/tx_logger.go",
    "test_count": 20,
    "benchmark_count": 0
  }
  Why: Would have confirmed test existence and count without grepping. Critical for "does X have tests?"
  queries.

  ---
  3. Symbol Index (Types, Functions, Interfaces)

  {
    "path": "internal/database/tx.go",
    "symbols": {
      "interfaces": ["DBTX", "TransactionMetrics"],
      "types": ["TxLogger"],
      "functions": ["NewTxLogger", "Begin", "Commit"]
    }
  }
  Why: I needed to verify "does TransactionMetrics exist?" - this would answer instantly.

  ---
  4. Comment Markers (TODO, FIXME, HACK, XXX)

  {
    "path": "main.go",
    "comment_markers": {
      "TODO": 3,
      "FIXME": 1,
      "HACK": 0
    }
  }
  Why: Automatic TODO tracking without reading files. Could generate TODO.md directly from manifest.

  ---
  Tier 2: Medium Effort, High Impact

  5. Import Graph

  {
    "path": "internal/requestingsource/service.go",
    "imports": [
      "github.com/.../internal/database",
      "github.com/.../internal/config"
    ],
    "imported_by_count": 5
  }
  Why: Understand dependencies and "blast radius" of changes. Find circular imports.

  ---
  6. Git Metadata (if .git exists)

  {
    "path": "internal/database/tx_logger.go",
    "git": {
      "last_commit": "2025-12-21",
      "last_author": "user@example.com",
      "commit_count": 15,
      "created": "2025-11-01"
    }
  }
  Why: TECHNICAL_DEBT.md claimed "Completed 2025-12-21" - git metadata would have verified this instantly.

  ---
  7. File Role Classification

  {
    "path": "internal/database/example_transaction.go",
    "role": "example",
    "indicators": ["//go:build ignore", "Example" in functions]
  }
  Roles: production, test, example, generated, migration, config, docs

  Why: TODO.md issue #2 is about example file location - auto-detection would surface this.

  ---
  Tier 3: Complex but Powerful

  8. Interface Implementation Map

  {
    "interfaces": {
      "database.DBTX": {
        "implementations": [
          "*pgxpool.Pool",
          "pgx.Tx"
        ],
        "should_implement": [
          "requestingsource.PostgresRepository",
          "requesttechnology.PostgresRepository"
        ]
      }
    }
  }
  Why: Would have immediately shown "DBTX exists but no repos use it" - the core issue.

  ---
  9. Test Coverage Map

  {
    "coverage_map": {
      "internal/database/tx_logger.go": {
        "test_file": "internal/database/tx_logger_test.go",
        "test_functions": 20,
        "coverage_estimate": "87.5%"
      },
      "internal/requestingsource/postgres.go": {
        "test_file": null,
        "test_functions": 0,
        "coverage_estimate": "0%"
      }
    }
  }
  Why: Instantly identify untested code. TECHNICAL_DEBT.md P0 issue.

  ---
  My Top 3 Recommendations (for immediate value)

  1. File Headers Snapshot (20-30 lines) - Solves 80% of struct/type verification
  2. Build Tags + Test Markers - Critical for test coverage analysis
  3. Symbol Index (interfaces, types, functions) - Answers "does X exist?" instantly

  These three would have reduced my token usage by another ~50% and made verification near-instantaneous.

  ---
  Bonus: Query Examples

  With these additions, queries become incredibly powerful:

  # Find all files implementing DBTX
  jq '.nodes[] | select(.symbols.implements[] == "DBTX") | .path'

  # Find untested production files
  jq '.nodes[] | select(.test_file == false and .test_for == null and .role == "production") | .path'

  # Find files with TODOs
  jq '.nodes[] | select(.comment_markers.TODO > 0) | {path, todos: .comment_markers.TODO}'

  # Find stale files (not modified in 6 months)
  jq '.nodes[] | select(.git.last_commit < "2025-07-01") | .path'

  # Find interface definitions with no implementations
  jq '.interfaces | to_entries[] | select(.value.implementations == []) | .key'
